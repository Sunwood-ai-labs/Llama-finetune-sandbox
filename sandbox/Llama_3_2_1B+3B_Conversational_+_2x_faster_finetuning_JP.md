
# Unsloth ã‚’ä½¿ã£ãŸé«˜é€Ÿãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€ã‹ã‚‰ã€Œã™ã¹ã¦ã‚’å®Ÿè¡Œã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚**ç„¡æ–™ã®** Tesla T4 Google Colab ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å‹•ä½œã—ã¾ã™ï¼

ã”è‡ªèº«ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«Unslothã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆã¯ã€[Githubãƒšãƒ¼ã‚¸ã®æ‰‹é †](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions)ã«å¾“ã£ã¦ãã ã•ã„ã€‚

ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å­¦ã³ã¾ã™ï¼š
- [ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™](#Data)æ–¹æ³• 
- [ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’](#Train)æ–¹æ³•
- [æ¨è«–ã®å®Ÿè¡Œ](#Inference)æ–¹æ³•
- [ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜](#Save)æ–¹æ³•(ä¾‹ï¼šLlama.cppç”¨)

**[NEW]** Llama-3.1 8b Instructã®2å€é«˜é€Ÿãªæ¨è«–ã‚’ç„¡æ–™Colabã§è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚[ã“ã¡ã‚‰](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)

ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ä¸»ãªæ©Ÿèƒ½ï¼š
1. Maxime Labonneã®[FineTome 100K](https://huggingface.co/datasets/mlabonne/FineTome-100k)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
2. ShareGPTã‹ã‚‰HuggingFaceå½¢å¼ã¸ã®å¤‰æ›(`standardize_sharegpt`ã‚’ä½¿ç”¨)
3. ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã®ã¿ã®å­¦ç¿’(`train_on_responses_only`ã‚’ä½¿ç”¨)
4. Unslothã¯Python 3.12ãŠã‚ˆã³Torch 2.4ã€ã™ã¹ã¦ã®TRL & Xformersãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆ

```python
%%capture
!pip install unsloth
# æœ€æ–°ã®UnslothãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒ“ãƒ«ãƒ‰ã‚’å–å¾—
!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ©Ÿèƒ½ï¼š
* Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermesãªã©å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«
* 16bit LoRAã¾ãŸã¯4bit QLoRAï¼ˆã©ã¡ã‚‰ã‚‚2å€é«˜é€Ÿï¼‰
* `max_seq_length`ã¯ä»»æ„ã®å€¤ã«è¨­å®šå¯èƒ½ï¼ˆ[kaiokendev](https://kaiokendev.github.io/til)ã®RoPEè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
* [**NEW**] Gemma-2 9b / 27bãŒ**2å€é«˜é€Ÿ**ã«ï¼[Gemma-2 9bã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)ã‚’ã”è¦§ãã ã•ã„
* [**NEW**] Ollamaã¸ã®è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯[Ollamaãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)ã‚’ãŠè©¦ã—ãã ã•ã„

```python
from unsloth import FastLanguageModel
import torch

# ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬è¨­å®š
max_seq_length = 2048  # ä»»æ„ã®é•·ã•ã‚’æŒ‡å®šå¯èƒ½ï¼ˆRoPEã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å†…éƒ¨ã§è‡ªå‹•å¯¾å¿œï¼‰
dtype = None  # è‡ªå‹•æ¤œå‡ºã€‚Tesla T4, V100ã¯Float16ã€Ampereä»¥é™ã¯Bfloat16
load_in_4bit = True  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹4bité‡å­åŒ–ã‚’ä½¿ç”¨ã€‚å¿…è¦ã«å¿œã˜ã¦Falseã«è¨­å®šå¯èƒ½

# 4bitäº‹å‰é‡å­åŒ–æ¸ˆã¿ã®å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä¸€è¦§ï¼ˆ4å€é€Ÿã„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2å€é€Ÿ
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 405bãƒ¢ãƒ‡ãƒ«ã®4bitç‰ˆ!
    "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2å€é€Ÿ!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2å€é€Ÿ!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2å€é€Ÿ!
    
    "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2ãƒ¢ãƒ‡ãƒ«ç¾¤
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
] # ã‚ˆã‚Šå¤šãã®ãƒ¢ãƒ‡ãƒ«ã¯ https://huggingface.co/unsloth ã§ç¢ºèªã§ãã¾ã™

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",  # ã¾ãŸã¯ "unsloth/Llama-3.2-1B-Instruct" ã‚’é¸æŠ
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...",  # meta-llama/Llama-2-7b-hf ãªã©ã®ã‚²ãƒ¼ãƒˆåˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦
)
```

ã“ã“ã§ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®1ã€œ10%ã®ã¿ã‚’æ›´æ–°ã™ã‚Œã°è‰¯ããªã‚Šã¾ã™ï¼

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,  # 0ã‚ˆã‚Šå¤§ãã„ä»»æ„ã®æ•°ã‚’é¸æŠå¯èƒ½ï¼ˆæ¨å¥¨å€¤: 8, 16, 32, 64, 128ï¼‰
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,  # ã©ã®å€¤ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ãŒã€0ãŒæœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™
    bias = "none",     # ã©ã®å€¤ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ãŒã€"none"ãŒæœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™
    use_gradient_checkpointing = "unsloth",  # é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã¯ True ã‹ "unsloth" ã‚’ä½¿ç”¨ï¼ˆ"unsloth"ã¯30%ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    random_state = 3407,
    use_rslora = False,   # Rank Stabilized LoRAã‚’ã‚µãƒãƒ¼ãƒˆ
    loftq_config = None,  # LoftQã‚‚ã‚µãƒãƒ¼ãƒˆ
)
```

## Data

### ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
ä¼šè©±å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯`Llama-3.1`å½¢å¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä»Šå›ã¯[Maxime Labonneã®FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ShareGPTå½¢å¼ã§ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€`("from", "value")`å½¢å¼ã§ã¯ãªãã€HuggingFaceã®æ¨™æº–çš„ãªãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å½¢å¼`("role", "content")`ã«å¤‰æ›ã—ã¾ã™ã€‚Llama-3ã¯ä¼šè©±ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¤ºã—ã¾ã™ï¼š

```
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>

I'm great thanks!<|eot_id|>
```

é©åˆ‡ãªãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã«`get_chat_template`é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3`ãªã©å¤šæ•°ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

```python
from unsloth.chat_templates import get_chat_template

# ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢é–¢æ•°
def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
from datasets import load_dataset
dataset = load_dataset("mlabonne/FineTome-100k", split = "train")
```

`standardize_sharegpt`ã‚’ä½¿ç”¨ã—ã¦ã€ShareGPTå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã®ä¸€èˆ¬çš„ãªå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ãªå½¢å¼ã®å¤‰æ›ãŒè¡Œã‚ã‚Œã¾ã™ï¼š

```python
# å¤‰æ›å‰ (ShareGPTå½¢å¼)ï¼š
{"from": "system", "value": "You are an assistant"}
{"from": "human", "value": "What is 2+2?"}
{"from": "gpt", "value": "It's 4."}

# å¤‰æ›å¾Œ (HuggingFaceå½¢å¼)ï¼š
{"role": "system", "content": "You are an assistant"}
{"role": "user", "content": "What is 2+2?"}
{"role": "assistant", "content": "It's 4."}
```

```python
from unsloth.chat_templates import standardize_sharegpt
dataset = standardize_sharegpt(dataset)
dataset = dataset.map(formatting_prompts_func, batched = True,)
```

ä¼šè©±ã®æ§‹é€ ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€5ç•ªç›®ã®ä¼šè©±ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
dataset[5]["conversations"]
```

ãã—ã¦ã€ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã“ã‚Œã‚‰ã®ä¼šè©±ã‚’ã©ã®ã‚ˆã†ã«å¤‰æ›ã—ãŸã‹ã‚’ç¢ºèªã—ã¾ã™ï¼š

**æ³¨æ„:** Llama 3.1 Instructã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã€`"Cutting Knowledge Date: December 2023\nToday Date: 26 July 2024"`ã‚’è‡ªå‹•çš„ã«è¿½åŠ ã—ã¾ã™ã€‚ã“ã‚Œã¯æ­£å¸¸ãªå‹•ä½œã§ã™ã€‚

```python
dataset[5]["text"]
```

## Train

### ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
HuggingFace TRLã®`SFTTrainer`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚è©³ç´°ã¯[TRL SFTãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/trl/sft_trainer)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã“ã“ã§ã¯å‡¦ç†é€Ÿåº¦ã‚’å„ªå…ˆã—ã¦60ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œã—ã¾ã™ãŒã€å®Œå…¨ãªå­¦ç¿’ã‚’è¡Œã†å ´åˆã¯`num_train_epochs=1`ã‚’è¨­å®šã—ã€`max_steps=None`ã¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€TRLã®`DPOTrainer`ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼

```python
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False,  # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’5å€é«˜é€ŸåŒ–ã§ãã¾ã™
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1,  # å®Œå…¨ãªå­¦ç¿’å®Ÿè¡Œã®å ´åˆã¯ã“ã¡ã‚‰ã‚’è¨­å®š
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",  # WandBãªã©ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã“ã“ã§è¨­å®š
    ),
)
```

Unslothã®`train_on_completions`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç„¡è¦–ã—ã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å‡ºåŠ›ã®ã¿ã‚’å­¦ç¿’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

```python
from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)
```

ãƒã‚¹ã‚­ãƒ³ã‚°ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
# å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºèª
tokenizer.decode(trainer.train_dataset[5]["input_ids"])
```

```python
# ãƒ©ãƒ™ãƒ«ã®ç¢ºèª
space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])
```

ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æŒ‡ç¤ºéƒ¨åˆ†ãŒæ­£ã—ããƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™ï¼

```python
#@title ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. æœ€å¤§ãƒ¡ãƒ¢ãƒª = {max_memory} GB.")
print(f"{start_gpu_memory} GBã®ãƒ¡ãƒ¢ãƒªãŒäºˆç´„æ¸ˆã¿.")
```

```python
# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ
trainer_stats = trainer.train()
```

```python
#@title æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªã¨æ™‚é–“ã®çµ±è¨ˆã‚’è¡¨ç¤º
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"å­¦ç¿’ã«ä½¿ç”¨ã—ãŸæ™‚é–“: {trainer_stats.metrics['train_runtime']} ç§’")
print(f"å­¦ç¿’ã«ä½¿ç”¨ã—ãŸæ™‚é–“: {round(trainer_stats.metrics['train_runtime']/60, 2)} åˆ†")
print(f"æœ€å¤§äºˆç´„ãƒ¡ãƒ¢ãƒª = {used_memory} GB")
print(f"å­¦ç¿’ç”¨ã®æœ€å¤§äºˆç´„ãƒ¡ãƒ¢ãƒª = {used_memory_for_lora} GB")
print(f"æœ€å¤§ãƒ¡ãƒ¢ãƒªã«å¯¾ã™ã‚‹ä½¿ç”¨ç‡ = {used_percentage} %")
print(f"å­¦ç¿’ç”¨ã®æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ = {lora_percentage} %")
```

## Inference

### æ¨è«–ã®å®Ÿè¡Œ
ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼æŒ‡ç¤ºã¨å…¥åŠ›ã¯å¤‰æ›´å¯èƒ½ã§ã™ã€‚å‡ºåŠ›ã¯ç©ºã®ã¾ã¾ã«ã—ã¦ãã ã•ã„ã€‚

**[NEW]** Llama-3.1 8b Instructã®2å€é«˜é€Ÿãªæ¨è«–ã‚’ç„¡æ–™Colabã§è©¦ã›ã¾ã™ã€‚[ã“ã¡ã‚‰](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)

`min_p = 0.1`ã¨`temperature = 1.5`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®è¨­å®šã«é–¢ã™ã‚‹è©³ç´°ã¯[ã“ã®ãƒ„ã‚¤ãƒ¼ãƒˆ](https://x.com/menhguin/status/1826132708508213629)ã‚’ã”è¦§ãã ã•ã„ã€‚

```python
from unsloth.chat_templates import get_chat_template

# ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å†è¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)
FastLanguageModel.for_inference(model)  # ãƒã‚¤ãƒ†ã‚£ãƒ–ã®2å€é€Ÿã„æ¨è«–ã‚’æœ‰åŠ¹åŒ–

# ãƒ†ã‚¹ãƒˆç”¨ã®ä¼šè©±ã‚’è¨­å®š
messages = [
    {"role": "user", "content": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True,  # ç”Ÿæˆã«ã¯å¿…é ˆ
    return_tensors = "pt",
).to("cuda")

# ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆ
outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,
                         temperature = 1.5, min_p = 0.1)
tokenizer.batch_decode(outputs)
```


`TextStreamer`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã‚’å¾…ã¤é–“ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«é€£ç¶šçš„ã«å‡ºåŠ›ã‚’ç¢ºèªã§ãã¾ã™ã€‚

```python
FastLanguageModel.for_inference(model)  # ãƒã‚¤ãƒ†ã‚£ãƒ–ã®2å€é€Ÿã„æ¨è«–ã‚’æœ‰åŠ¹åŒ–

messages = [
    {"role": "user", "content": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True,  # ç”Ÿæˆã«ã¯å¿…é ˆ
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
```

## Save

### å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨ãƒ­ãƒ¼ãƒ‰
LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã—ã¦æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã«ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¿å­˜ç”¨ã®HuggingFaceã®`push_to_hub`ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ç”¨ã®`save_pretrained`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**æ³¨æ„:** ã“ã‚Œã¯LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜ã—ã€å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã—ã¾ã›ã‚“ã€‚16bitã¾ãŸã¯GGUFã¨ã—ã¦ä¿å­˜ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®èª¬æ˜ã‚’ã”è¦§ãã ã•ã„ã€‚

```python
# ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")
# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¿å­˜
# model.push_to_hub("your_name/lora_model", token = "...")
# tokenizer.push_to_hub("your_name/lora_model", token = "...")
```

ä¿å­˜ã—ãŸLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ¨è«–ç”¨ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ã€`False`ã‚’`True`ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š

```python
if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model",  # å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)  # ãƒã‚¤ãƒ†ã‚£ãƒ–ã®2å€é€Ÿã„æ¨è«–ã‚’æœ‰åŠ¹åŒ–

# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
messages = [
    {"role": "user", "content": "Describe a tall tower in the capital of France."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True,  # ç”Ÿæˆã«ã¯å¿…é ˆ
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
```

ã¾ãŸã€HuggingFaceã®`AutoModelForPeftCausalLM`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãŸã ã—ã€ã“ã‚Œã¯`unsloth`ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚`4bit`ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚‰ãšã€Unslothã®æ¨è«–ãŒ2å€é€Ÿã„ãŸã‚ã€éå¸¸ã«é…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### VLLMã®ãŸã‚ã®float16å½¢å¼ã§ã®ä¿å­˜

ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥`float16`å½¢å¼ã§ä¿å­˜ã™ã‚‹ã“ã¨ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚float16ã®å ´åˆã¯`merged_16bit`ã‚’ã€int4ã®å ´åˆã¯`merged_4bit`ã‚’é¸æŠã—ã¾ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦`lora`ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯`push_to_hub_merged`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã¯ https://huggingface.co/settings/tokens ã§å–å¾—ã§ãã¾ã™ã€‚

```python
# 16bitå½¢å¼ã§ãƒãƒ¼ã‚¸
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# 4bitå½¢å¼ã§ãƒãƒ¼ã‚¸
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿
if False: model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")
```

### GGUF / llama.cppã¸ã®å¤‰æ›

`GGUF` / `llama.cpp`å½¢å¼ã§ã®ä¿å­˜ã‚‚æ¨™æº–ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼`llama.cpp`ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`q8_0`å½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚`q4_k_m`ãªã©ã®ä»–ã®å½¢å¼ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã«ã¯`save_pretrained_gguf`ã‚’ã€HFã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«ã¯`push_to_hub_gguf`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹é‡å­åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå®Œå…¨ãªãƒªã‚¹ãƒˆã¯[Wikiãƒšãƒ¼ã‚¸](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)ã‚’å‚ç…§ï¼‰ï¼š
* `q8_0` - é«˜é€Ÿãªå¤‰æ›ã€‚ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã¯å¤šã„ãŒã€ä¸€èˆ¬çš„ã«è¨±å®¹ç¯„å›²
* `q4_k_m` - æ¨å¥¨ã€‚attention.wvã¨feed_forward.w2ãƒ†ãƒ³ã‚½ãƒ«ã®åŠåˆ†ã«Q6_Kã‚’ä½¿ç”¨ã—ã€æ®‹ã‚Šã«Q4_Kã‚’ä½¿ç”¨
* `q5_k_m` - æ¨å¥¨ã€‚attention.wvã¨feed_forward.w2ãƒ†ãƒ³ã‚½ãƒ«ã®åŠåˆ†ã«Q6_Kã‚’ä½¿ç”¨ã—ã€æ®‹ã‚Šã«Q5_Kã‚’ä½¿ç”¨

[**NEW**] Ollamaã¸ã®è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯[Ollamaãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚

```python
# 8bit Q8_0å½¢å¼ã§ä¿å­˜
if False: model.save_pretrained_gguf("model", tokenizer,)
# ãƒˆãƒ¼ã‚¯ãƒ³ã¯ https://huggingface.co/settings/tokens ã§å–å¾—ã—ã¦ãã ã•ã„ï¼
# hfã¯ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# 16bit GGUFå½¢å¼ã§ä¿å­˜
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# q4_k_m GGUFå½¢å¼ã§ä¿å­˜
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# è¤‡æ•°ã®GGUFå½¢å¼ã§ä¿å­˜ - è¤‡æ•°å½¢å¼ãŒå¿…è¦ãªå ´åˆã¯ã“ã¡ã‚‰ãŒé«˜é€Ÿ
if False:
    model.push_to_hub_gguf(
        "hf/model",  # hfã‚’ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",  # ãƒˆãƒ¼ã‚¯ãƒ³ã¯ https://huggingface.co/settings/tokens ã§å–å¾—
    )
```

å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆ`model-unsloth.gguf`ã¾ãŸã¯`model-unsloth-Q4_K_M.gguf`ï¼‰ã¯ã€`llama.cpp`ã‚„`GPT4All`ãªã©ã®UIãƒ™ãƒ¼ã‚¹ã®ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã§ãã¾ã™ã€‚GPT4Allã¯[å…¬å¼ã‚µã‚¤ãƒˆ](https://gpt4all.io/index.html)ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½ã§ã™ã€‚

**[NEW]** Llama-3.1 8b Instructã®2å€é«˜é€Ÿãªæ¨è«–ã‚’ç„¡æ–™Colabã§è©¦ã›ã¾ã™ã€‚[ã“ã¡ã‚‰](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)ã‹ã‚‰

ä»¥ä¸Šã§çµ‚äº†ã§ã™ï¼Unslothã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚‹å ´åˆã¯ã€[Discord](https://discord.gg/u54VK8m8tk)ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚ãƒã‚°ã®å ±å‘Šã‚„æœ€æ–°ã®LLMæƒ…å ±ã®å…¥æ‰‹ã€ã‚µãƒãƒ¼ãƒˆã®è¦è«‹ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®å‚åŠ ãªã©ã¯Discordã§å—ã‘ä»˜ã‘ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®å‚è€ƒãƒªãƒ³ã‚¯ï¼š
1. Zephyr DPO 2å€é€Ÿ [ç„¡æ–™Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7b 2å€é€Ÿ [ç„¡æ–™Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlama 4å€é€Ÿ Alpaca 52Kå®Œå…¨ç‰ˆï¼ˆ1æ™‚é–“ï¼‰ [ç„¡æ–™Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34b 2å€é€Ÿ [Colabã®A100ç‰ˆ](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7b [Kaggleç‰ˆ](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. ğŸ¤— HuggingFaceã¨ã®[ãƒ–ãƒ­ã‚°è¨˜äº‹](https://huggingface.co/blog/unsloth-trl)ã€TRL[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)ã«ã‚‚æ²è¼‰
7. ShareGPTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®`ChatML`ã€[ä¼šè©±ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)
8. å°èª¬åŸ·ç­†ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ[ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. [**NEW**] Phi-3 Medium / Mini ãŒ**2å€é€Ÿ**ã«ï¼[Phi-3 Mediumãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
10. [**NEW**] Gemma-2 9b / 27b ãŒ**2å€é€Ÿ**ã«ï¼[Gemma-2 9bãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)
11. [**NEW**] Ollamaã¸ã®è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°[Ollamaãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)
12. [**NEW**] Mistral NeMo 12BãŒ2å€é€Ÿã«ï¼12GBæœªæº€ã®VRAMã§å‹•ä½œï¼[Mistral NeMoãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)
