# ğŸ¦™ Unslothã§ä½œæˆã—ãŸLLaMA 3.2ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸé«˜é€Ÿæ¨è«–ã‚¬ã‚¤ãƒ‰

## ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
%%capture
!pip install unsloth
# æœ€æ–°ã®UnslothãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒ“ãƒ«ãƒ‰ã‚’å–å¾—
!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

**è§£èª¬**:
Unslothãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€LLaMAãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’å¤§å¹…ã«é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚ãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒ“ãƒ«ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æœ€æ–°ã®æ©Ÿèƒ½ã¨æ”¹å–„ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚

## ğŸ”§ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åŸºæœ¬è¨­å®š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template
import torch

# ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬è¨­å®š
max_seq_length = 512
dtype = None
load_in_4bit = True
model_id = "MakiAi/Llama-3-2-3B-Instruct-bnb-4bit-10epochs-adapter"  # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
```

**è§£èª¬**:
- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- ãƒ¢ãƒ‡ãƒ«ã¯4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æ”¹å–„
- `model_id`ã«ã¯ã€Unslothã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š

## ğŸš€ ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–

```python
# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_id,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    trust_remote_code=True,
)

# LLaMA 3.1ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨
tokenizer = get_chat_template(
    tokenizer,
    chat_template="llama-3.1",  # LLaMA 3.1ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§å•é¡Œãªã—
)

# é«˜é€Ÿæ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
FastLanguageModel.for_inference(model)  # é€šå¸¸ã®2å€ã®é€Ÿåº¦
```

**è§£èª¬**:
1. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
2. LLaMA 3.1ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ï¼ˆ3.2ã§ã‚‚äº’æ›æ€§ã‚ã‚Šï¼‰
3. Unslothã®é«˜é€Ÿæ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–

## ğŸ’¬ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸæ¨è«–ã®å®Ÿè£…

```python
def generate_response(dataset_entry):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã«å¯¾ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½œæˆ
    messages = [
        {"role": "user", "content": dataset_entry["conversations"][0]['content']},
    ]

    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é©ç”¨
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,  # ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¿½åŠ 
        return_tensors="pt",
    ).to(model.device)

    # å¿œç­”ã®ç”Ÿæˆ
    outputs = model.generate(
        input_ids=inputs,
        max_new_tokens=64,  # ç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        use_cache=True,     # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦é«˜é€ŸåŒ–
        temperature=1.5,    # ã‚ˆã‚Šå‰µé€ çš„ãªå¿œç­”ã‚’ç”Ÿæˆ
        min_p=0.1          # å‡ºåŠ›ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿
    )

    return tokenizer.batch_decode(outputs)
```

**è§£èª¬**:
ã“ã®é–¢æ•°ã¯ï¼š
1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’æŠ½å‡º
2. LLaMA 3.1å½¢å¼ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
3. ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¿œç­”ã‚’ç”Ÿæˆï¼š
   - `max_new_tokens`: 64ï¼ˆçŸ­ã‚ã®å¿œç­”ã‚’ç”Ÿæˆï¼‰
   - `temperature`: 1.5ï¼ˆå‰µé€ æ€§ã‚’é«˜ã‚ã‚‹ï¼‰
   - `min_p`: 0.1ï¼ˆå¤šæ§˜ãªå¿œç­”ã‚’ç¢ºä¿ï¼‰

## âœ… å®Ÿè¡Œä¾‹

```python
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = [
        {"conversations": [{"content": "ç«ç„”çŒ«ç‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"}]},
        {"conversations": [{"content": "æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£ã®æœ¬è³ªã¯ä½•ã§ã™ã‹ï¼Ÿ"}]},
        {"conversations": [{"content": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"}]}
    ]

    # 2ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã§è©¦ã—ã¦ã¿ã‚‹
    response = generate_response(dataset[0])

    print("å…¥åŠ›:", dataset[0]["conversations"][0]['content'])
    print("å¿œç­”:", response)
```

```python
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = [
        {"conversations": [{"content": "ç«ç„”çŒ«ç‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"}]},
        {"conversations": [{"content": "æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£ã®æœ¬è³ªã¯ä½•ã§ã™ã‹ï¼Ÿ"}]},
        {"conversations": [{"content": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"}]}
    ]

    # 2ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã§è©¦ã—ã¦ã¿ã‚‹
    response = generate_response(dataset[1])

    print("å…¥åŠ›:", dataset[1]["conversations"][0]['content'])
    print("å¿œç­”:", response)
```

```python
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = [
        {"conversations": [{"content": "ç«ç„”çŒ«ç‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"}]},
        {"conversations": [{"content": "æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£ã®æœ¬è³ªã¯ä½•ã§ã™ã‹ï¼Ÿ"}]},
        {"conversations": [{"content": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"}]}
    ]

    # 2ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã§è©¦ã—ã¦ã¿ã‚‹
    response = generate_response(dataset[2])

    print("å…¥åŠ›:", dataset[2]["conversations"][0]['content'])
    print("å¿œç­”:", response)
```

**è§£èª¬**:
ã‚µãƒ³ãƒ—ãƒ«ã®å®Ÿè¡Œæ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š
- ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®šç¾©
- é¸æŠã—ãŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã§å¿œç­”ã‚’ç”Ÿæˆ
- å…¥åŠ›ã¨ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’è¡¨ç¤º

ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€Unslothã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚«ã‚¹ã‚¿ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸLLaMA 3.2ãƒ¢ãƒ‡ãƒ«ã‚’ã€é«˜é€Ÿã«æ¨è«–ã§ãã¾ã™ã€‚LLaMA 3.1ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å®‰å®šã—ãŸå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ç‰¹æ€§ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚
